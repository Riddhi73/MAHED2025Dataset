{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Noik9q9c7Bhm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# [Task 3: Multimodal Hate Speech Detection in Memes](https://github.com/marsadlab/MAHED2025Dataset/tree/main/task3/) at [ArabicNLP 2025](http://arabicnlp2025.sigarab.org/) @EMNLP 2025\n",
    "\n",
    "\n",
    "Given multimodal content (text extracted from meme and the meme itself) the task is to detect whether the content is hateful or not-hateful. This is a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYZ96DWt-TZk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### installing required libraries.\n",
    " - transformers\n",
    " - datasets\n",
    " - evaluate\n",
    " - accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SLJh5GGU-xET",
    "outputId": "7ff1646f-157f-4922-8d81-88d4208074c9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install --upgrade accelerate\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXhVWUJ3A_hx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### importing required libraries and setting up logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIUAU0rRBOmR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor, Resize, CenterCrop\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    ConvNextFeatureExtractor,\n",
    "    ResNetConfig,\n",
    "    ResNetForImageClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-_w4YehCgX4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setting up the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-GUUNj0BPbu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    output_dir=\"./resnet_50/\",\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=False,\n",
    "    local_rank= 1,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "max_train_samples = None\n",
    "max_eval_samples=None\n",
    "max_predict_samples=None\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Q4deAnUJ0iI",
    "outputId": "2afd566e-4e46-44ce-9119-ec3fba8af95c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgkvwlbFHVo5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-De1tz5qHYre",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'resnet50'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPqrrDbcKN8n",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### setting the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvKpoxaQKTB6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HP6CdL7NHpxJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Download data from HF: https://huggingface.co/datasets/QCRI/Prop2Hate-Meme\n",
    "### Defining the training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167,
     "referenced_widgets": [
      "377fb13dd4164f01bacc36fa240bbefc",
      "00de5bb85b9c4256b9df6f4aaa6f6ec3",
      "df1602a259fb425386bb56a7b19093f9",
      "9894335180b44f69b3fa70ea72dc68b9",
      "b60b6c0a21c5437692aa9cf6f9bbb295",
      "1530195c8ad34ce597f2b4b37211b2e6",
      "94598dfd63b74186b7297310ca7c35a5",
      "6a05973aa49b4313ab38bbbaaa84b3c6",
      "8c3317c749e14f50baea4a6b1248c03a",
      "82fdeef31fff4818ab5b11430d693755",
      "46dcfd0698b24bf7bddf0cfcf9a1035a",
      "a16622549b2b477898a4bc6e2a3f902f",
      "05cc1f1d527a4846a7a564ac03436c1c",
      "be6abd29665042a6bea6d14ed4155da1",
      "28c7868ce174421c9997737a286364b0",
      "36aea1dad4f34f55a03645ff32c3f680",
      "0978d65bf7d94785a7d9952584b43fd1",
      "e8c77ab2439242778069878af5070723",
      "844efe96034145b48fc57933369bc015",
      "644eb710d6434cb2ae9aba49266163bb",
      "2bf816fe3581466685e885c00077bdc1",
      "2401cfb6f0cd40c9943ec42fb7c732ab",
      "2d99c9b8c20d40a9827cb2b2825f6cbc",
      "62c98e8d40174502abeabeb8aa5323cd",
      "345511dd7d794712b89df0b5be5ec492",
      "05cbbea18d524ceb9dadd519a5d62505",
      "6c143f56e39441d49466cabbc9b72184",
      "90c5c6e485384d51a4aafae7e74e90d2",
      "768adda801334042bbc87e9525a69cb1",
      "24ef5c625a6e47bd95e9d04826b9dd1a",
      "bfda13ed28074ebda35a90f68b571e56",
      "9c34c84bf04c4ed2bcdab8aeeb3816f2",
      "9902ced509d04b5db1dc41bdc10daa6b"
     ]
    },
    "id": "rP5NzWz1QYrk",
    "outputId": "9122466e-9882-4afa-fc20-339b83d92a59"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"QCRI/Prop2Hate-Meme\")\n",
    "\n",
    "# Specify the directory where you want to save the dataset\n",
    "\n",
    "output_dir=\"./Prop2Hate-Meme\"\n",
    "\n",
    "# Save the dataset to the specified directory. This will save all splits to the output directory.\n",
    "dataset.save_to_disk(output_dir)\n",
    "\n",
    "# If you want to get the raw images from HF dataset format\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Directory to save the images\n",
    "output_dir=\"./Prop2Hate-Meme/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over the dataset and save each image\n",
    "for split in ['train','dev','test']:\n",
    "    jsonl_path = os.path.join(output_dir, f\"arabic_hateful_meme_{split}.jsonl\")\n",
    "    with open(jsonl_path, 'w', encoding='utf-8') as f:\n",
    "        for idx, item in enumerate(dataset[split]):\n",
    "            # Access the image directly as it's already a PIL.Image object\n",
    "            image = item['image']\n",
    "            image_path = os.path.join(output_dir, item['img_path'])\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(os.path.dirname(image_path), exist_ok=True)\n",
    "            image.save(image_path)\n",
    "            del item['image']\n",
    "            del item['prop_label']\n",
    "            del item['hate_fine_grained_label']\n",
    "            item['label'] = item.pop('hate_label')\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMzfE34iHyGV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"Prop2Hate-Meme\")\n",
    "\n",
    "train_file = './arabic_hateful_meme_train.jsonl'\n",
    "validation_file = './arabic_hateful_meme_dev.jsonl'\n",
    "test_file = './arabic_hateful_meme_test.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_2mAbjfSp7s",
    "outputId": "ca9c28f4-ba4e-4cf1-e603-ca4d2c4207a6"
   },
   "outputs": [],
   "source": [
    "jsonl_path = \"./arabic_hateful_meme_train.jsonl\" # Example path, modify as needed\n",
    "data = []\n",
    "with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "  for line in f:\n",
    "    data.append(json.loads(line))\n",
    "\n",
    "# data is now a list of dictionaries, where each dictionary is a parsed JSON object from a line in the file.\n",
    "print(f\"Loaded {len(data)} entries from {jsonl_path}\")\n",
    "if data:\n",
    "    print(\"First entry:\")\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgNrs7AhKdvl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Loading data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDwaW8AnKcgD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Image\n",
    "\n",
    "def read_jsonl_to_df(filename):\n",
    "    return pd.read_json(filename, lines=True)\n",
    "\n",
    "l2id = {'not-hate': 0, 'hate': 1}\n",
    "\n",
    "# Assume all splits use \"img_path\" as the image column\n",
    "def prepare_dataset(file):\n",
    "    df = read_jsonl_to_df(file)\n",
    "    # df['label'] = df['label'].map(l2id)\n",
    "    # Cast \"img_path\" column as Image\n",
    "    return Dataset.from_pandas(df).cast_column(\"img_path\", Image())\n",
    "\n",
    "train_dataset = prepare_dataset(train_file)\n",
    "validation_dataset = prepare_dataset(validation_file)\n",
    "test_dataset = prepare_dataset(test_file)\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\"train\": train_dataset, \"validation\": validation_dataset, \"test\": test_dataset}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJhNu7tPQ2RU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Extracting number of unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTl6NNPmOXhO",
    "outputId": "87c4b2c7-88d6-441b-83a2-8db0863ad6ed",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Labels\n",
    "label_list = raw_datasets[\"train\"].unique(\"label\")\n",
    "label_list.sort()  # sort the labels for determine\n",
    "num_labels = len(label_list)\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1dpoOAPRJnN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading Pretrained Configuration, Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmAaMuBuRQd2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = ResNetConfig(\n",
    "        num_channels=1,\n",
    "        layer_type=\"basic\",\n",
    "        depths=[2, 2],\n",
    "        hidden_sizes=[32, 64],\n",
    "        num_labels=num_labels,\n",
    ")\n",
    "\n",
    "model = ResNetForImageClassification(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7PIQVypeTf4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Preprocessing the raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPUy5k7ac0E7",
    "outputId": "5149a935-9e3e-4d26-cb9b-3824a6826aa9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_extractor = ConvNextFeatureExtractor(\n",
    "    do_resize=True, do_normalize=False, image_mean=[0.45], image_std=[0.22]\n",
    ")\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "_transforms = Compose([Resize(256), CenterCrop(224), ToTensor(), normalize])\n",
    "\n",
    "def transforms(example_batch):\n",
    "    \"\"\"Apply _train_transforms across a batch.\"\"\"\n",
    "    # print(example_batch)\n",
    "    # black and white\n",
    "    example_batch[\"pixel_values\"] = [_transforms(pil_img.convert(\"L\")) for pil_img in example_batch[\"img_path\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASxWKiqifb_g",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Finalize the training data for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHoDqrBGgD6F",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if \"train\" not in raw_datasets:\n",
    "    raise ValueError(\"requires a train dataset\")\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "if max_train_samples is not None:\n",
    "    max_train_samples_n = min(len(train_dataset), max_train_samples)\n",
    "    train_dataset = train_dataset.select(range(max_train_samples_n))\n",
    "train_dataset.set_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqME25nm-hwo",
    "outputId": "bd373cc0-c6b7-48e0-8dd7-b5fe32619c49",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k72vUTSigOzZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Finalize the development/evaluation data for evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqrW8ospgUYZ",
    "outputId": "7c190a5e-4b7f-44f9-ae98-b1494b18feba",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if \"validation\" not in raw_datasets:\n",
    "    raise ValueError(\"requires a validation dataset\")\n",
    "eval_dataset = raw_datasets[\"validation\"]\n",
    "if max_eval_samples is not None:\n",
    "    max_eval_samples_n = min(len(eval_dataset), max_eval_samples)\n",
    "    eval_dataset = eval_dataset.select(range(max_eval_samples_n))\n",
    "eval_dataset.set_transform(transforms)\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7sVqp3hgU4i",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Finalize the test data for predicting the unseen test data using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0dBjIQggcYs",
    "outputId": "a60370c3-7b5c-4d0c-a491-eff190a4fc92",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if \"test\" not in raw_datasets and \"test_matched\" not in raw_datasets:\n",
    "    raise ValueError(\"requires a test dataset\")\n",
    "predict_dataset = raw_datasets[\"test\"]\n",
    "if max_predict_samples is not None:\n",
    "    max_predict_samples_n = min(len(predict_dataset), max_predict_samples)\n",
    "    predict_dataset = predict_dataset.select(range(max_predict_samples_n))\n",
    "predict_dataset.set_transform(transforms)\n",
    "predict_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cqbo1xzRge36",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Log a few random samples from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIO2bxSVgkLb",
    "outputId": "d82a8491-b5b6-40d3-d381-eba44cfaef0c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAcn0Pc8gogF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Get the metric function `accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMWMQdaUgvAq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foWUyuBHgxbA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Predictions and label_ids field and has to return a dictionary string to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3VqxkqcgxCC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNWK1Hfbg8-o",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_w6lNh-OhJLC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    # print(examples)\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "data_collator = collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nYlugPRhNbg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Initialize our Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yeJco0JOhPHx",
    "outputId": "0c42d0ea-f4b1-46b6-9534-e4ee9c54214c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset, # if you have development and test set, uncomment this line\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUxWn9HrhqRM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "B681qnPFhtY0",
    "outputId": "e57149bf-3d33-4f2b-861d-4ca0d182f51d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "max_train_samples = (\n",
    "    max_train_samples if max_train_samples is not None else len(train_dataset)\n",
    ")\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaaRglkwllSp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Saving the tokenizer too for easy upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9UwoMEbAloMx",
    "outputId": "8149d18c-0bc9-488f-b851-62bea17282a1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9zCKBGEhwb7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Evaluating our model on validation/development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "YClw3dXTh17u",
    "outputId": "01699aa5-2ec0-4abc-c04f-7b489a143032",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "max_eval_samples = (\n",
    "    max_eval_samples if max_eval_samples is not None else len(eval_dataset)\n",
    ")\n",
    "metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3LSdUdPh7uG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Predecting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71INKaEmU6lS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# if the test set is available, you don't need to run this cell\n",
    "# predict_dataset = eval_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "gnXhVq6Yh_oS",
    "outputId": "9761365c-c6e9-4aa1-cecb-d332c5f4d3ea",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "id2l = {0:'not-hate', 1:'hate'}\n",
    "logger.info(\"*** Predict ***\")\n",
    "\n",
    "predictions = trainer.predict(predict_dataset, metric_key_prefix=\"predict\").predictions\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "output_predict_file = os.path.join(training_args.output_dir, f\"task2_TeamName.csv\")\n",
    "if trainer.is_world_process_zero():\n",
    "    with open(output_predict_file, \"w\") as writer:\n",
    "        logger.info(f\"***** Predict results *****\")\n",
    "        writer.write(\"id\\tprediction\\n\")\n",
    "        for index, item in enumerate(predictions):\n",
    "            item = label_list[item]\n",
    "            item = id2l[item]\n",
    "            writer.write(f\"{predict_dataset[index]['id']}\\t{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQgoTTIoiI0X",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Saving the model into card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1ooJgrViLVj",
    "outputId": "db72f3bd-7001-42c0-a8ae-84e8b512f10f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kwargs = {\"finetuned_from\": model_name, \"tasks\": \"image-classification\"}\n",
    "trainer.create_model_card(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbSWop7viib9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
